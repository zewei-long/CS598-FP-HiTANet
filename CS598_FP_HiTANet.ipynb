{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS598-FP-HiTANet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import torch.nn.init as init\n",
        "import copy\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "xGMPzAo_xHSq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer architecture"
      ],
      "metadata": {
        "id": "_1bed9a4HjTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(torch.nn.Embedding):\n",
        "\n",
        "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None,\n",
        "                 max_norm=None, norm_type=2., scale_grad_by_freq=False,\n",
        "                 sparse=False, _weight=None):\n",
        "        super(Embedding, self).__init__(num_embeddings, embedding_dim, padding_idx=padding_idx,\n",
        "                                        max_norm=max_norm, norm_type=norm_type, scale_grad_by_freq=scale_grad_by_freq,\n",
        "                                        sparse=sparse, _weight=_weight)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.padding_idx is not None:\n",
        "            with torch.no_grad():\n",
        "                self.weight[self.padding_idx].fill_(0)\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"Scaled dot-product attention mechanism.\"\"\"\n",
        "\n",
        "    def __init__(self, attention_dropout=0.0):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, q, k, v, scale=None, attn_mask=None):\n",
        "        attention = torch.bmm(q, k.transpose(1, 2))\n",
        "        if scale:\n",
        "            attention = attention * scale\n",
        "        if attn_mask is not None:\n",
        "            attention = attention.masked_fill_(attn_mask, -np.inf)\n",
        "        attention = self.softmax(attention)\n",
        "        attention = self.dropout(attention)\n",
        "        context = torch.bmm(attention, v)\n",
        "        return context, attention\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_seq_len):\n",
        "\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "\n",
        "        position_encoding = np.array([\n",
        "            [pos / np.power(10000, 2.0 * (j // 2) / d_model) for j in range(d_model)]\n",
        "            for pos in range(max_seq_len)])\n",
        "\n",
        "        position_encoding[:, 0::2] = np.sin(position_encoding[:, 0::2])\n",
        "        position_encoding[:, 1::2] = np.cos(position_encoding[:, 1::2])\n",
        "        position_encoding = torch.from_numpy(position_encoding.astype(np.float32))\n",
        "\n",
        "        pad_row = torch.zeros([1, d_model])\n",
        "        position_encoding = torch.cat((pad_row, position_encoding))\n",
        "\n",
        "        self.position_encoding = nn.Embedding(max_seq_len + 1, d_model)\n",
        "        self.position_encoding.weight = nn.Parameter(position_encoding,\n",
        "                                                     requires_grad=False)\n",
        "\n",
        "    def forward(self, input_len):\n",
        "\n",
        "\n",
        "        max_len = torch.max(input_len)\n",
        "        tensor = torch.cuda.LongTensor if input_len.is_cuda else torch.LongTensor\n",
        "\n",
        "        pos = np.zeros([len(input_len), max_len])\n",
        "        for ind, length in enumerate(input_len):\n",
        "            for pos_ind in range(1, length + 1):\n",
        "                pos[ind, pos_ind - 1] = pos_ind\n",
        "        input_pos = tensor(pos)\n",
        "        return self.position_encoding(input_pos), input_pos\n",
        "\n",
        "\n",
        "class PositionalWiseFeedForward(nn.Module):\n",
        "    def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0):\n",
        "        super(PositionalWiseFeedForward, self).__init__()\n",
        "        self.w1 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
        "        self.w2 = nn.Conv1d(ffn_dim, model_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(model_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x.transpose(1, 2)\n",
        "        output = self.w2(F.relu(self.w1(output)))\n",
        "        output = self.dropout(output.transpose(1, 2))\n",
        "\n",
        "        # add residual and norm layer\n",
        "        output = self.layer_norm(x + output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, model_dim=512, num_heads=8, dropout=0.0):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.dim_per_head = model_dim // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.linear_k = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
        "        self.linear_v = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
        "        self.linear_q = nn.Linear(model_dim, self.dim_per_head * num_heads)\n",
        "\n",
        "        self.dot_product_attention = ScaledDotProductAttention(dropout)\n",
        "        self.linear_final = nn.Linear(model_dim, model_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(model_dim)\n",
        "\n",
        "    def forward(self, key, value, query, attn_mask=None):\n",
        "        residual = query\n",
        "\n",
        "        dim_per_head = self.dim_per_head\n",
        "        num_heads = self.num_heads\n",
        "        batch_size = key.size(0)\n",
        "\n",
        "        # linear projection\n",
        "        key = self.linear_k(key)\n",
        "        value = self.linear_v(value)\n",
        "        query = self.linear_q(query)\n",
        "\n",
        "        # split by heads\n",
        "        key = key.view(batch_size * num_heads, -1, dim_per_head)\n",
        "        value = value.view(batch_size * num_heads, -1, dim_per_head)\n",
        "        query = query.view(batch_size * num_heads, -1, dim_per_head)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn_mask = attn_mask.repeat(num_heads, 1, 1)\n",
        "        # scaled dot product attention\n",
        "        scale = (key.size(-1) // num_heads) ** -0.5\n",
        "        context, attention = self.dot_product_attention(\n",
        "            query, key, value, scale, attn_mask)\n",
        "\n",
        "        # concat heads\n",
        "        context = context.view(batch_size, -1, dim_per_head * num_heads)\n",
        "\n",
        "        # final linear projection\n",
        "        output = self.linear_final(context)\n",
        "\n",
        "        # dropout\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        # add residual and norm layer\n",
        "        output = self.layer_norm(residual + output)\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, model_dim=512, num_heads=8, ffn_dim=2018, dropout=0.0):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(model_dim, num_heads, dropout)\n",
        "        self.feed_forward = PositionalWiseFeedForward(model_dim, ffn_dim, dropout)\n",
        "\n",
        "    def forward(self, inputs, attn_mask=None):\n",
        "        # self attention\n",
        "        context, attention = self.attention(inputs, inputs, inputs, attn_mask)\n",
        "\n",
        "        # feed forward network\n",
        "        output = self.feed_forward(context)\n",
        "\n",
        "        return output, attention\n",
        "\n",
        "\n",
        "def padding_mask(seq_k, seq_q):\n",
        "    len_q = seq_q.size(1)\n",
        "    pad_mask = seq_k.eq(0)\n",
        "    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)  # shape [B, L_q, L_k]\n",
        "    return pad_mask\n",
        "\n",
        "\n",
        "def padding_mask_sand(seq_k, seq_q):\n",
        "    len_q = seq_q.size(1)\n",
        "    pad_mask = seq_k.eq(0)\n",
        "    pad_mask = pad_mask.unsqueeze(1).expand(-1, len_q, -1)  # shape [B, L_q, L_k]\n",
        "    return pad_mask\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 max_seq_len,\n",
        "                 num_layers=1,\n",
        "                 model_dim=256,\n",
        "                 num_heads=4,\n",
        "                 ffn_dim=1024,\n",
        "                 dropout=0.0):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(model_dim, num_heads, ffn_dim, dropout) for _ in\n",
        "             range(num_layers)])\n",
        "        self.pre_embedding = Embedding(vocab_size, model_dim)\n",
        "        self.bias_embedding = torch.nn.Parameter(torch.Tensor(model_dim))\n",
        "        bound = 1 / math.sqrt(vocab_size)\n",
        "        init.uniform_(self.bias_embedding, -bound, bound)\n",
        "\n",
        "        # self.weight_layer = torch.nn.Linear(model_dim, 1)\n",
        "        self.pos_embedding = PositionalEncoding(model_dim, max_seq_len)\n",
        "        self.time_layer = torch.nn.Linear(64, 256)\n",
        "        self.selection_layer = torch.nn.Linear(1, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, diagnosis_codes, mask, mask_code, seq_time_step, input_len):\n",
        "        seq_time_step = torch.Tensor(seq_time_step).cuda().unsqueeze(2) / 180\n",
        "        time_feature = 1 - self.tanh(torch.pow(self.selection_layer(seq_time_step), 2))\n",
        "        time_feature = self.time_layer(time_feature)\n",
        "        output = (self.pre_embedding(diagnosis_codes) * mask_code).sum(dim=2) + self.bias_embedding\n",
        "        output += time_feature\n",
        "        output_pos, ind_pos = self.pos_embedding(input_len.unsqueeze(1))\n",
        "        output += output_pos\n",
        "        self_attention_mask = padding_mask(ind_pos, ind_pos)\n",
        "\n",
        "        attentions = []\n",
        "        outputs = []\n",
        "        for encoder in self.encoder_layers:\n",
        "            output, attention = encoder(output, self_attention_mask)\n",
        "            attentions.append(attention)\n",
        "            outputs.append(output)\n",
        "        # weight = torch.softmax(self.weight_layer(outputs[-1]), dim=1)\n",
        "        # weight = weight * mask - 255 * (1 - mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "def adjust_input(batch_diagnosis_codes, batch_time_step, max_len, n_diagnosis_codes):\n",
        "    batch_time_step = copy.deepcopy(batch_time_step)\n",
        "    batch_diagnosis_codes = copy.deepcopy(batch_diagnosis_codes)\n",
        "    for ind in range(len(batch_diagnosis_codes)):\n",
        "        if len(batch_diagnosis_codes[ind]) > max_len:\n",
        "            batch_diagnosis_codes[ind] = batch_diagnosis_codes[ind][-(max_len):]\n",
        "            batch_time_step[ind] = batch_time_step[ind][-(max_len):]\n",
        "        batch_time_step[ind].append(0)\n",
        "        batch_diagnosis_codes[ind].append([n_diagnosis_codes - 1])\n",
        "    return batch_diagnosis_codes, batch_time_step\n",
        "\n",
        "class TimeEncoder(nn.Module):\n",
        "    def __init__(self, batch_size):\n",
        "        super(TimeEncoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.selection_layer = torch.nn.Linear(1, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.weight_layer = torch.nn.Linear(64, 64)\n",
        "\n",
        "    def forward(self, seq_time_step, final_queries, options, mask):\n",
        "        if options['use_gpu']:\n",
        "            seq_time_step = torch.Tensor(seq_time_step).unsqueeze(2).cuda() / 180\n",
        "        else:\n",
        "            seq_time_step = torch.Tensor(seq_time_step).unsqueeze(2) / 180\n",
        "        selection_feature = 1 - self.tanh(torch.pow(self.selection_layer(seq_time_step), 2))\n",
        "        selection_feature = self.relu(self.weight_layer(selection_feature))\n",
        "        selection_feature = torch.sum(selection_feature * final_queries, 2, keepdim=True) / 8\n",
        "        selection_feature = selection_feature.masked_fill_(mask, -np.inf)\n",
        "        # time_weights = self.weight_layer(selection_feature)\n",
        "        return torch.softmax(selection_feature, 1)\n",
        "\n",
        "\n",
        "class TransformerTime(nn.Module):\n",
        "    def __init__(self, n_diagnosis_codes, batch_size, options):\n",
        "        super(TransformerTime, self).__init__()\n",
        "        # self.prior_encoder = PriorEncoder(batch_size, options)\n",
        "        self.time_encoder = TimeEncoder(batch_size)\n",
        "        self.feature_encoder = Encoder(options['n_diagnosis_codes'] + 1, 51, num_layers=options['layer'])\n",
        "        self.self_layer = torch.nn.Linear(256, 1)\n",
        "        self.classify_layer = torch.nn.Linear(256, 2)\n",
        "        self.quiry_layer = torch.nn.Linear(256, 64)\n",
        "        self.quiry_weight_layer = torch.nn.Linear(256, 2)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # dropout layer\n",
        "        dropout_rate = options['dropout_rate']\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def get_self_attention(self, features, query, mask):\n",
        "        attention = torch.softmax(self.self_layer(features).masked_fill(mask, -np.inf), dim=1)\n",
        "        # attention = torch.sum(key * query, 2, keepdim=True) / 8\n",
        "        return attention\n",
        "\n",
        "    def forward(self, seq_dignosis_codes, seq_time_step, batch_labels, options, maxlen):\n",
        "        # seq_dignosis_codes: [batch_size, length, bag_len]\n",
        "        # seq_time_step: [batch_size, length] the day times to the final visit\n",
        "        # batch_labels: [batch_size] 0 negative 1 positive\n",
        "        seq_time_step = np.array(list(pad_time(seq_time_step, options)))\n",
        "        lengths = torch.from_numpy(np.array([len(seq) for seq in seq_dignosis_codes])).cuda()\n",
        "        diagnosis_codes, labels, mask, mask_final, mask_code = pad_matrix_new(seq_dignosis_codes,\n",
        "                                                                                        batch_labels, options)\n",
        "        if options['use_gpu']:\n",
        "            diagnosis_codes = torch.LongTensor(diagnosis_codes).cuda()\n",
        "            mask_mult = torch.BoolTensor(1-mask).unsqueeze(2).cuda()\n",
        "            mask_final = torch.Tensor(mask_final).unsqueeze(2).cuda()\n",
        "            mask_code = torch.Tensor(mask_code).unsqueeze(3).cuda()\n",
        "        else:\n",
        "            diagnosis_codes = torch.LongTensor(diagnosis_codes)\n",
        "            mask_mult = torch.BoolTensor(1-mask).unsqueeze(2)\n",
        "            mask_final = torch.Tensor(mask_final).unsqueeze(2)\n",
        "            mask_code = torch.Tensor(mask_code).unsqueeze(3)\n",
        "        features = self.feature_encoder(diagnosis_codes, mask_mult, mask_code, seq_time_step, lengths)\n",
        "        final_statues = features * mask_final\n",
        "        final_statues = final_statues.sum(1, keepdim=True)\n",
        "        quiryes = self.relu(self.quiry_layer(final_statues))\n",
        "\n",
        "        self_weight = self.get_self_attention(features, quiryes, mask_mult)\n",
        "        time_weight = self.time_encoder(seq_time_step, quiryes, options, mask_mult)\n",
        "        attention_weight = torch.softmax(self.quiry_weight_layer(final_statues), 2)\n",
        "\n",
        "        total_weight = torch.cat((time_weight, self_weight), 2)\n",
        "        total_weight = torch.sum(total_weight * attention_weight, 2, keepdim=True)\n",
        "        total_weight = total_weight / (torch.sum(total_weight, 1, keepdim=True) + 1e-5)\n",
        "        weighted_features = features * total_weight\n",
        "        averaged_features = torch.sum(weighted_features, 1)\n",
        "        averaged_features = self.dropout(averaged_features)\n",
        "        predictions = self.classify_layer(averaged_features)\n",
        "        labels = torch.LongTensor(labels)\n",
        "        if options['use_gpu']:\n",
        "            labels = labels.cuda()\n",
        "        return predictions, labels, self_weight"
      ],
      "metadata": {
        "id": "LEbS6-LdZr3w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "about data processing before training"
      ],
      "metadata": {
        "id": "c_6WJFDcH6AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(training_file, validation_file, testing_file):\n",
        "    train = np.array(pickle.load(open(training_file, 'rb')))\n",
        "    validate = np.array(pickle.load(open(validation_file, 'rb')))\n",
        "    test = np.array(pickle.load(open(testing_file, 'rb')))\n",
        "    return train, validate, test\n",
        "\n",
        "def cut_data(training_file, validation_file, testing_file):\n",
        "    train = list(pickle.load(open(training_file, 'rb')))\n",
        "    validate = list(pickle.load(open(validation_file, 'rb')))\n",
        "    test = list(pickle.load(open(testing_file, 'rb')))\n",
        "    for dataset in [train, validate, test]:\n",
        "        dataset[0] = dataset[0][0: len(dataset[0]) // 18]\n",
        "        dataset[1] = dataset[1][0: len(dataset[1]) // 18]\n",
        "        dataset[2] = dataset[2][0: len(dataset[2]) // 18]\n",
        "    return train, validate, test\n",
        "\n",
        "\n",
        "def pad_time(seq_time_step, options):\n",
        "    lengths = np.array([len(seq) for seq in seq_time_step])\n",
        "    maxlen = np.max(lengths)\n",
        "    for k in range(len(seq_time_step)):\n",
        "        while len(seq_time_step[k]) < maxlen:\n",
        "            seq_time_step[k].append(100000)\n",
        "\n",
        "    return seq_time_step\n",
        "\n",
        "def pad_matrix_new(seq_diagnosis_codes, seq_labels, options):\n",
        "    lengths = np.array([len(seq) for seq in seq_diagnosis_codes])\n",
        "    n_samples = len(seq_diagnosis_codes)\n",
        "    n_diagnosis_codes = options['n_diagnosis_codes']\n",
        "    maxlen = np.max(lengths)\n",
        "    lengths_code = []\n",
        "    for seq in seq_diagnosis_codes:\n",
        "        for code_set in seq:\n",
        "            lengths_code.append(len(code_set))\n",
        "    lengths_code = np.array(lengths_code)\n",
        "    maxcode = np.max(lengths_code)\n",
        "\n",
        "    batch_diagnosis_codes = np.zeros((n_samples, maxlen, maxcode), dtype=np.int64) + options['n_diagnosis_codes']\n",
        "    batch_mask = np.zeros((n_samples, maxlen), dtype=np.float32)\n",
        "    batch_mask_code = np.zeros((n_samples, maxlen, maxcode), dtype=np.float32)\n",
        "    batch_mask_final = np.zeros((n_samples, maxlen), dtype=np.float32)\n",
        "\n",
        "    for bid, seq in enumerate(seq_diagnosis_codes):\n",
        "        for pid, subseq in enumerate(seq):\n",
        "            for tid, code in enumerate(subseq):\n",
        "                batch_diagnosis_codes[bid, pid, tid] = code\n",
        "                batch_mask_code[bid, pid, tid] = 1\n",
        "\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        batch_mask[i, 0:lengths[i]-1] = 1\n",
        "        max_visit = lengths[i] - 1\n",
        "        batch_mask_final[i, max_visit] = 1\n",
        "\n",
        "    batch_labels = np.array(seq_labels, dtype=np.int64)\n",
        "\n",
        "    return batch_diagnosis_codes, batch_labels, batch_mask, batch_mask_final, batch_mask_code\n",
        "\n",
        "\n",
        "def calculate_cost_tran(model, data, options, max_len, loss_function=F.cross_entropy):\n",
        "    model.eval()\n",
        "    batch_size = options['batch_size']\n",
        "    n_batches = int(np.ceil(float(len(data[0])) / float(batch_size)))\n",
        "    cost_sum = 0.0\n",
        "\n",
        "    for index in range(n_batches):\n",
        "        batch_diagnosis_codes = data[0][batch_size * index: batch_size * (index + 1)]\n",
        "        batch_time_step = data[2][batch_size * index: batch_size * (index + 1)]\n",
        "        batch_diagnosis_codes, batch_time_step = adjust_input(batch_diagnosis_codes, batch_time_step, max_len, options['n_diagnosis_codes'])\n",
        "        batch_labels = data[1][batch_size * index: batch_size * (index + 1)]\n",
        "        lengths = np.array([len(seq) for seq in batch_diagnosis_codes])\n",
        "        maxlen = np.max(lengths)\n",
        "        logit, labels, self_attention = model(batch_diagnosis_codes, batch_time_step, batch_labels, options, maxlen)\n",
        "        loss = loss_function(logit, labels)\n",
        "        cost_sum += loss.cpu().data.numpy()\n",
        "    model.train()\n",
        "    return cost_sum / n_batches\n",
        "\n",
        "\n",
        "def adjust_input(batch_diagnosis_codes, batch_time_step, max_len, n_diagnosis_codes):\n",
        "    batch_time_step = copy.deepcopy(batch_time_step)\n",
        "    batch_diagnosis_codes = copy.deepcopy(batch_diagnosis_codes)\n",
        "    for ind in range(len(batch_diagnosis_codes)):\n",
        "        if len(batch_diagnosis_codes[ind]) > max_len:\n",
        "            batch_diagnosis_codes[ind] = batch_diagnosis_codes[ind][-(max_len):]\n",
        "            batch_time_step[ind] = batch_time_step[ind][-(max_len):]\n",
        "        batch_time_step[ind].append(0)\n",
        "        batch_diagnosis_codes[ind].append([n_diagnosis_codes-1])\n",
        "    return batch_diagnosis_codes, batch_time_step\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "        This criterion is a implemenation of Focal Loss, which is proposed in\n",
        "        Focal Loss for Dense Object Detection.\n",
        "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
        "        The losses are averaged across observations for each minibatch.\n",
        "        Args:\n",
        "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
        "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5),\n",
        "                                   putting more focus on hard, misclassiﬁed examples\n",
        "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
        "                                However, if the field size_average is set to False, the losses are\n",
        "                                instead summed for each minibatch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        if alpha is None:\n",
        "            self.alpha = Variable(torch.ones(class_num, 1))\n",
        "        else:\n",
        "            if isinstance(alpha, Variable):\n",
        "                self.alpha = alpha\n",
        "            else:\n",
        "                self.alpha = Variable(alpha)\n",
        "        self.gamma = gamma\n",
        "        self.class_num = class_num\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        N = inputs.size(0)\n",
        "        C = inputs.size(1)\n",
        "        P = nn.functional.softmax(inputs)\n",
        "\n",
        "        class_mask = inputs.data.new(N, C).fill_(0)\n",
        "        class_mask = Variable(class_mask)\n",
        "        ids = targets.view(-1, 1)\n",
        "        class_mask.scatter_(1, ids.data, 1.)\n",
        "        # print(class_mask)\n",
        "\n",
        "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
        "            self.alpha = self.alpha.cuda()\n",
        "        alpha = self.alpha[ids.data.view(-1)]\n",
        "\n",
        "        probs = (P * class_mask).sum(1).view(-1, 1)\n",
        "\n",
        "        log_p = probs.log()\n",
        "        # print('probs size= {}'.format(probs.size()))\n",
        "        # print(probs)\n",
        "\n",
        "        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p\n",
        "        # print('-----bacth_loss------')\n",
        "        # print(batch_loss)\n",
        "\n",
        "        if self.size_average:\n",
        "            loss = batch_loss.mean()\n",
        "        else:\n",
        "            loss = batch_loss.sum()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "sK7kb8eKaITY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model training and testing function"
      ],
      "metadata": {
        "id": "EXSx6QC9H-jm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l4QciC7gZO2k"
      },
      "outputs": [],
      "source": [
        "def train_model(training_file='training_file',\n",
        "                validation_file='validation_file',\n",
        "                testing_file='testing_file',\n",
        "                n_diagnosis_codes=10000,\n",
        "                n_labels=2,\n",
        "                batch_size=100,\n",
        "                dropout_rate=0.5,\n",
        "                L2_reg=0.001,\n",
        "                n_epoch=1000,\n",
        "                log_eps=1e-8,\n",
        "                visit_size=512,\n",
        "                hidden_size=256,\n",
        "                use_gpu=False,\n",
        "                model_name='',\n",
        "                disease = 'hf',\n",
        "                code2id = None,\n",
        "                running_data='',\n",
        "                gamma=0.5,\n",
        "                model_file = None,\n",
        "                layer=1):\n",
        "    options = locals().copy()\n",
        "\n",
        "    print('building the model ...')\n",
        "    model = model_file(n_diagnosis_codes, batch_size, options)\n",
        "    focal_loss = FocalLoss(2, gamma=gamma)\n",
        "    print('constructing the optimizer ...')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay = options['L2_reg'])\n",
        "    print('done!')\n",
        "\n",
        "    print('loading data ...')\n",
        "    train, validate, test = load_data(training_file, validation_file, testing_file)\n",
        "    n_batches = int(np.ceil(float(len(train[0])) / float(batch_size)))\n",
        "\n",
        "    print('training start')\n",
        "    best_train_cost = 0.0\n",
        "    best_validate_cost = 100000000.0\n",
        "    best_test_cost = 0.0\n",
        "    epoch_duaration = 0.0\n",
        "    best_epoch = 0.0\n",
        "    max_len = 50\n",
        "    best_parameters_file = ''\n",
        "    if use_gpu:\n",
        "        model.cuda()\n",
        "    model.train()\n",
        "    for epoch in range(n_epoch):\n",
        "        iteration = 0\n",
        "        cost_vector = []\n",
        "        start_time = time.time()\n",
        "        samples = random.sample(range(n_batches), n_batches)\n",
        "        counter = 0\n",
        "\n",
        "        for index in samples:\n",
        "            batch_diagnosis_codes = train[0][batch_size * index: batch_size * (index + 1)]\n",
        "            batch_time_step = train[2][batch_size * index: batch_size * (index + 1)]\n",
        "            batch_diagnosis_codes, batch_time_step = adjust_input(batch_diagnosis_codes, batch_time_step, max_len, n_diagnosis_codes)\n",
        "            batch_labels = train[1][batch_size * index: batch_size * (index + 1)]\n",
        "            lengths = np.array([len(seq) for seq in batch_diagnosis_codes])\n",
        "            maxlen = np.max(lengths)\n",
        "            predictions, labels, self_attention = model(batch_diagnosis_codes, batch_time_step, batch_labels, options, maxlen)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = focal_loss(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            cost_vector.append(loss.cpu().data.numpy())\n",
        "\n",
        "            if (iteration % 50 == 0):\n",
        "                print('epoch:%d, iteration:%d/%d, cost:%f' % (epoch, iteration, n_batches, loss.cpu().data.numpy()))\n",
        "                #print(self_attention[:,0,0].squeeze().cpu().data.numpy())\n",
        "                #print(time_weight[:, 0])\n",
        "                #print(prior_weight[:, 0])\n",
        "                #print(model.time_encoder.time_weight[0:10])\n",
        "                #print(self_weight[:, 0])\n",
        "            iteration += 1\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "        print('epoch:%d, mean_cost:%f, duration:%f' % (epoch, np.mean(cost_vector), duration))\n",
        "\n",
        "        train_cost = np.mean(cost_vector)\n",
        "        validate_cost = calculate_cost_tran(model, validate, options, max_len, focal_loss)\n",
        "        test_cost = calculate_cost_tran(model, test, options, max_len, focal_loss)\n",
        "        print('epoch:%d, validate_cost:%f, duration:%f' % (epoch, validate_cost, duration))\n",
        "        epoch_duaration += duration\n",
        "\n",
        "        train_cost = np.mean(cost_vector)\n",
        "        epoch_duaration += duration\n",
        "        if validate_cost > (best_validate_cost + 0.04) and epoch > 19:\n",
        "            print(validate_cost)\n",
        "            print(best_validate_cost)\n",
        "            break\n",
        "        if validate_cost < best_validate_cost:\n",
        "            best_validate_cost = validate_cost\n",
        "            best_train_cost = train_cost\n",
        "            best_test_cost = test_cost\n",
        "            best_epoch = epoch\n",
        "        buf = 'Best Epoch:%d, Train_Cost:%f, Valid_Cost:%f, Test_Cost:%f' % (\n",
        "        best_epoch, best_train_cost, best_validate_cost, best_test_cost)\n",
        "        print(buf)\n",
        "    # testing\n",
        "    model.eval()\n",
        "    n_batches = int(np.ceil(float(len(test[0])) / float(batch_size)))\n",
        "    y_true = np.array([])\n",
        "    y_pred = np.array([])\n",
        "    for index in range(n_batches):\n",
        "        batch_diagnosis_codes = test[0][batch_size * index: batch_size * (index + 1)]\n",
        "        batch_time_step = test[2][batch_size * index: batch_size * (index + 1)]\n",
        "        batch_diagnosis_codes, batch_time_step = adjust_input(batch_diagnosis_codes, batch_time_step, max_len, n_diagnosis_codes)\n",
        "        batch_labels = test[1][batch_size * index: batch_size * (index + 1)]\n",
        "        lengths = np.array([len(seq) for seq in batch_diagnosis_codes])\n",
        "        maxlen = np.max(lengths)\n",
        "        logit, labels, self_attention = model(batch_diagnosis_codes, batch_time_step, batch_labels, options, maxlen)\n",
        "\n",
        "        if use_gpu:\n",
        "            prediction = torch.max(logit, 1)[1].view((len(labels),)).data.cpu().numpy()\n",
        "            labels = labels.data.cpu().numpy()\n",
        "        else:\n",
        "            prediction = torch.max(logit, 1)[1].view((len(labels),)).data.numpy()\n",
        "            labels = labels.data.numpy()\n",
        "\n",
        "        y_true = np.concatenate((y_true, labels))\n",
        "        y_pred = np.concatenate((y_pred, prediction))\n",
        "\n",
        "    accuary = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred)\n",
        "    print(accuary, precision, recall, f1, roc_auc)\n",
        "    return (accuary, precision, recall, f1, roc_auc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "main function and hyperparameter etc."
      ],
      "metadata": {
        "id": "Fidp3sAvIGNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "batch_size = 50\n",
        "dropout_rate = 0.5\n",
        "L2_reg = 1e-3\n",
        "log_eps = 1e-8\n",
        "n_epoch = 20\n",
        "n_labels = 2  # binary classification\n",
        "visit_size = 256 # size of input embedding\n",
        "hidden_size = 256 # size of hidden layer\n",
        "gamma = 0.0 # setting for Focal Loss, when it's zero, it's equal to standard cross loss\n",
        "use_gpu = True\n",
        "layer = 1 # layer of Transformer\n",
        "model_choice = 'TransformerTime' # name of the proposed HiTANet in our paper\n",
        "model_file = eval(model_choice)\n",
        "disease_list = ['hf_sample'] # name of the sample data set, you can place you own data set by following the same setting\n",
        "for disease in disease_list:\n",
        "    model_name = 'tran_%s_%s_L%d_wt_1e-4_focal%.2f' % (model_choice, disease, layer, gamma)\n",
        "    print(model_name)\n",
        "    log_file = 'results/' + model_name + '.txt'\n",
        "    path = './'\n",
        "    trianing_file = path + disease + '_training_new.pickle'\n",
        "    validation_file = path + disease + '_validation_new.pickle'\n",
        "    testing_file = path + disease + '_testing_new.pickle'\n",
        "\n",
        "    dict_file = './' + disease + '_code2idx_new.pickle'\n",
        "    code2id = pickle.load(open(dict_file, 'rb'))\n",
        "    n_diagnosis_codes = len(pickle.load(open(dict_file, 'rb'))) + 1\n",
        "\n",
        "    results = []\n",
        "    for k in range(10):\n",
        "        accuary, precision, recall, f1, roc_auc = train_model(trianing_file, validation_file,\n",
        "                                                              testing_file, n_diagnosis_codes, n_labels,\n",
        "                                                              batch_size, dropout_rate,\n",
        "                                                              L2_reg, n_epoch, log_eps, visit_size, hidden_size,\n",
        "                                                              use_gpu, model_name, disease=disease, code2id=None,\n",
        "                                                              gamma=gamma, layer=layer, model_file=model_file)\n",
        "        results.append([accuary, precision, recall, f1, roc_auc])\n",
        "\n",
        "    results = np.array(results)\n",
        "    print(np.mean(results, 0))"
      ],
      "metadata": {
        "id": "UxrSXKIiZoO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c310160-95e3-4f3c-e67e-a50f416b5a43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tran_TransformerTime_hf_sample_L1_wt_1e-4_focal0.00\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:1.093137\n",
            "epoch:0, mean_cost:0.718268, duration:0.521142\n",
            "epoch:0, validate_cost:0.615147, duration:0.521142\n",
            "Best Epoch:0, Train_Cost:0.718268, Valid_Cost:0.615147, Test_Cost:0.585229\n",
            "epoch:1, iteration:0/11, cost:0.610077\n",
            "epoch:1, mean_cost:0.643056, duration:0.169584\n",
            "epoch:1, validate_cost:0.584463, duration:0.169584\n",
            "Best Epoch:1, Train_Cost:0.643056, Valid_Cost:0.584463, Test_Cost:0.535284\n",
            "epoch:2, iteration:0/11, cost:0.696982\n",
            "epoch:2, mean_cost:0.619244, duration:0.184096\n",
            "epoch:2, validate_cost:0.584967, duration:0.184096\n",
            "Best Epoch:1, Train_Cost:0.643056, Valid_Cost:0.584463, Test_Cost:0.535284\n",
            "epoch:3, iteration:0/11, cost:0.677091\n",
            "epoch:3, mean_cost:0.607125, duration:0.165046\n",
            "epoch:3, validate_cost:0.559759, duration:0.165046\n",
            "Best Epoch:3, Train_Cost:0.607125, Valid_Cost:0.559759, Test_Cost:0.466958\n",
            "epoch:4, iteration:0/11, cost:0.514873\n",
            "epoch:4, mean_cost:0.564242, duration:0.174027\n",
            "epoch:4, validate_cost:0.555509, duration:0.174027\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:5, iteration:0/11, cost:0.631642\n",
            "epoch:5, mean_cost:0.593453, duration:0.174486\n",
            "epoch:5, validate_cost:0.557348, duration:0.174486\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:6, iteration:0/11, cost:0.630342\n",
            "epoch:6, mean_cost:0.533806, duration:0.163954\n",
            "epoch:6, validate_cost:0.557312, duration:0.163954\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:7, iteration:0/11, cost:0.493266\n",
            "epoch:7, mean_cost:0.578298, duration:0.155968\n",
            "epoch:7, validate_cost:0.559826, duration:0.155968\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:8, iteration:0/11, cost:0.526905\n",
            "epoch:8, mean_cost:0.561135, duration:0.156586\n",
            "epoch:8, validate_cost:0.561025, duration:0.156586\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:9, iteration:0/11, cost:0.618793\n",
            "epoch:9, mean_cost:0.558588, duration:0.161978\n",
            "epoch:9, validate_cost:0.565387, duration:0.161978\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:10, iteration:0/11, cost:0.657862\n",
            "epoch:10, mean_cost:0.566681, duration:0.166298\n",
            "epoch:10, validate_cost:0.564526, duration:0.166298\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:11, iteration:0/11, cost:0.428084\n",
            "epoch:11, mean_cost:0.553699, duration:0.161842\n",
            "epoch:11, validate_cost:0.560820, duration:0.161842\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:12, iteration:0/11, cost:0.603215\n",
            "epoch:12, mean_cost:0.532196, duration:0.164549\n",
            "epoch:12, validate_cost:0.567843, duration:0.164549\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:13, iteration:0/11, cost:0.633542\n",
            "epoch:13, mean_cost:0.526651, duration:0.160471\n",
            "epoch:13, validate_cost:0.569949, duration:0.160471\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:14, iteration:0/11, cost:0.614401\n",
            "epoch:14, mean_cost:0.533736, duration:0.157570\n",
            "epoch:14, validate_cost:0.577298, duration:0.157570\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:15, iteration:0/11, cost:0.307333\n",
            "epoch:15, mean_cost:0.527493, duration:0.166206\n",
            "epoch:15, validate_cost:0.570880, duration:0.166206\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:16, iteration:0/11, cost:0.558302\n",
            "epoch:16, mean_cost:0.504769, duration:0.161719\n",
            "epoch:16, validate_cost:0.577286, duration:0.161719\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:17, iteration:0/11, cost:0.564147\n",
            "epoch:17, mean_cost:0.505398, duration:0.164866\n",
            "epoch:17, validate_cost:0.588309, duration:0.164866\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:18, iteration:0/11, cost:0.476291\n",
            "epoch:18, mean_cost:0.492210, duration:0.154161\n",
            "epoch:18, validate_cost:0.580916, duration:0.154161\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "epoch:19, iteration:0/11, cost:0.508209\n",
            "epoch:19, mean_cost:0.486165, duration:0.159223\n",
            "epoch:19, validate_cost:0.584252, duration:0.159223\n",
            "Best Epoch:4, Train_Cost:0.564242, Valid_Cost:0.555509, Test_Cost:0.435730\n",
            "0.7352941176470589 0.8311688311688312 0.8205128205128205 0.8258064516129032 0.6394230769230769\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:1.339991\n",
            "epoch:0, mean_cost:0.836897, duration:0.161524\n",
            "epoch:0, validate_cost:0.726748, duration:0.161524\n",
            "Best Epoch:0, Train_Cost:0.836897, Valid_Cost:0.726748, Test_Cost:0.685892\n",
            "epoch:1, iteration:0/11, cost:0.719826\n",
            "epoch:1, mean_cost:0.611255, duration:0.157146\n",
            "epoch:1, validate_cost:0.609267, duration:0.157146\n",
            "Best Epoch:1, Train_Cost:0.611255, Valid_Cost:0.609267, Test_Cost:0.557238\n",
            "epoch:2, iteration:0/11, cost:0.695062\n",
            "epoch:2, mean_cost:0.605388, duration:0.158425\n",
            "epoch:2, validate_cost:0.590363, duration:0.158425\n",
            "Best Epoch:2, Train_Cost:0.605388, Valid_Cost:0.590363, Test_Cost:0.509543\n",
            "epoch:3, iteration:0/11, cost:0.635151\n",
            "epoch:3, mean_cost:0.601033, duration:0.164512\n",
            "epoch:3, validate_cost:0.577725, duration:0.164512\n",
            "Best Epoch:3, Train_Cost:0.601033, Valid_Cost:0.577725, Test_Cost:0.475743\n",
            "epoch:4, iteration:0/11, cost:0.556510\n",
            "epoch:4, mean_cost:0.592878, duration:0.166793\n",
            "epoch:4, validate_cost:0.571766, duration:0.166793\n",
            "Best Epoch:4, Train_Cost:0.592878, Valid_Cost:0.571766, Test_Cost:0.457840\n",
            "epoch:5, iteration:0/11, cost:0.668194\n",
            "epoch:5, mean_cost:0.592074, duration:0.164423\n",
            "epoch:5, validate_cost:0.590623, duration:0.164423\n",
            "Best Epoch:4, Train_Cost:0.592878, Valid_Cost:0.571766, Test_Cost:0.457840\n",
            "epoch:6, iteration:0/11, cost:0.713091\n",
            "epoch:6, mean_cost:0.566648, duration:0.160587\n",
            "epoch:6, validate_cost:0.571287, duration:0.160587\n",
            "Best Epoch:6, Train_Cost:0.566648, Valid_Cost:0.571287, Test_Cost:0.460801\n",
            "epoch:7, iteration:0/11, cost:0.578374\n",
            "epoch:7, mean_cost:0.573125, duration:0.166491\n",
            "epoch:7, validate_cost:0.573790, duration:0.166491\n",
            "Best Epoch:6, Train_Cost:0.566648, Valid_Cost:0.571287, Test_Cost:0.460801\n",
            "epoch:8, iteration:0/11, cost:0.695129\n",
            "epoch:8, mean_cost:0.571093, duration:0.164643\n",
            "epoch:8, validate_cost:0.569491, duration:0.164643\n",
            "Best Epoch:8, Train_Cost:0.571093, Valid_Cost:0.569491, Test_Cost:0.449097\n",
            "epoch:9, iteration:0/11, cost:0.499198\n",
            "epoch:9, mean_cost:0.563899, duration:0.161643\n",
            "epoch:9, validate_cost:0.568689, duration:0.161643\n",
            "Best Epoch:9, Train_Cost:0.563899, Valid_Cost:0.568689, Test_Cost:0.447467\n",
            "epoch:10, iteration:0/11, cost:0.549324\n",
            "epoch:10, mean_cost:0.549068, duration:0.175138\n",
            "epoch:10, validate_cost:0.566824, duration:0.175138\n",
            "Best Epoch:10, Train_Cost:0.549068, Valid_Cost:0.566824, Test_Cost:0.440838\n",
            "epoch:11, iteration:0/11, cost:0.516805\n",
            "epoch:11, mean_cost:0.559502, duration:0.161718\n",
            "epoch:11, validate_cost:0.561575, duration:0.161718\n",
            "Best Epoch:11, Train_Cost:0.559502, Valid_Cost:0.561575, Test_Cost:0.435607\n",
            "epoch:12, iteration:0/11, cost:0.509499\n",
            "epoch:12, mean_cost:0.559289, duration:0.164865\n",
            "epoch:12, validate_cost:0.574552, duration:0.164865\n",
            "Best Epoch:11, Train_Cost:0.559502, Valid_Cost:0.561575, Test_Cost:0.435607\n",
            "epoch:13, iteration:0/11, cost:0.345642\n",
            "epoch:13, mean_cost:0.528755, duration:0.162329\n",
            "epoch:13, validate_cost:0.560254, duration:0.162329\n",
            "Best Epoch:13, Train_Cost:0.528755, Valid_Cost:0.560254, Test_Cost:0.434396\n",
            "epoch:14, iteration:0/11, cost:0.508477\n",
            "epoch:14, mean_cost:0.533677, duration:0.161752\n",
            "epoch:14, validate_cost:0.563862, duration:0.161752\n",
            "Best Epoch:13, Train_Cost:0.528755, Valid_Cost:0.560254, Test_Cost:0.434396\n",
            "epoch:15, iteration:0/11, cost:0.486165\n",
            "epoch:15, mean_cost:0.531149, duration:0.164155\n",
            "epoch:15, validate_cost:0.559743, duration:0.164155\n",
            "Best Epoch:15, Train_Cost:0.531149, Valid_Cost:0.559743, Test_Cost:0.431923\n",
            "epoch:16, iteration:0/11, cost:0.431898\n",
            "epoch:16, mean_cost:0.532462, duration:0.162515\n",
            "epoch:16, validate_cost:0.559824, duration:0.162515\n",
            "Best Epoch:15, Train_Cost:0.531149, Valid_Cost:0.559743, Test_Cost:0.431923\n",
            "epoch:17, iteration:0/11, cost:0.449641\n",
            "epoch:17, mean_cost:0.516125, duration:0.164702\n",
            "epoch:17, validate_cost:0.563436, duration:0.164702\n",
            "Best Epoch:15, Train_Cost:0.531149, Valid_Cost:0.559743, Test_Cost:0.431923\n",
            "epoch:18, iteration:0/11, cost:0.414273\n",
            "epoch:18, mean_cost:0.509391, duration:0.156992\n",
            "epoch:18, validate_cost:0.559581, duration:0.156992\n",
            "Best Epoch:18, Train_Cost:0.509391, Valid_Cost:0.559581, Test_Cost:0.421099\n",
            "epoch:19, iteration:0/11, cost:0.335341\n",
            "epoch:19, mean_cost:0.510310, duration:0.156519\n",
            "epoch:19, validate_cost:0.563568, duration:0.156519\n",
            "Best Epoch:18, Train_Cost:0.509391, Valid_Cost:0.559581, Test_Cost:0.421099\n",
            "0.7058823529411765 0.8157894736842105 0.7948717948717948 0.8051948051948051 0.6057692307692307\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:0.805206\n",
            "epoch:0, mean_cost:0.656886, duration:0.176244\n",
            "epoch:0, validate_cost:0.644394, duration:0.176244\n",
            "Best Epoch:0, Train_Cost:0.656886, Valid_Cost:0.644394, Test_Cost:0.580495\n",
            "epoch:1, iteration:0/11, cost:0.693329\n",
            "epoch:1, mean_cost:0.612284, duration:0.248539\n",
            "epoch:1, validate_cost:0.581106, duration:0.248539\n",
            "Best Epoch:1, Train_Cost:0.612284, Valid_Cost:0.581106, Test_Cost:0.497080\n",
            "epoch:2, iteration:0/11, cost:0.619714\n",
            "epoch:2, mean_cost:0.588146, duration:0.155883\n",
            "epoch:2, validate_cost:0.573918, duration:0.155883\n",
            "Best Epoch:2, Train_Cost:0.588146, Valid_Cost:0.573918, Test_Cost:0.473334\n",
            "epoch:3, iteration:0/11, cost:0.615752\n",
            "epoch:3, mean_cost:0.585191, duration:0.164303\n",
            "epoch:3, validate_cost:0.574623, duration:0.164303\n",
            "Best Epoch:2, Train_Cost:0.588146, Valid_Cost:0.573918, Test_Cost:0.473334\n",
            "epoch:4, iteration:0/11, cost:0.537727\n",
            "epoch:4, mean_cost:0.593026, duration:0.166455\n",
            "epoch:4, validate_cost:0.565176, duration:0.166455\n",
            "Best Epoch:4, Train_Cost:0.593026, Valid_Cost:0.565176, Test_Cost:0.452311\n",
            "epoch:5, iteration:0/11, cost:0.622800\n",
            "epoch:5, mean_cost:0.562237, duration:0.161790\n",
            "epoch:5, validate_cost:0.568082, duration:0.161790\n",
            "Best Epoch:4, Train_Cost:0.593026, Valid_Cost:0.565176, Test_Cost:0.452311\n",
            "epoch:6, iteration:0/11, cost:0.495946\n",
            "epoch:6, mean_cost:0.575748, duration:0.155616\n",
            "epoch:6, validate_cost:0.566493, duration:0.155616\n",
            "Best Epoch:4, Train_Cost:0.593026, Valid_Cost:0.565176, Test_Cost:0.452311\n",
            "epoch:7, iteration:0/11, cost:0.669966\n",
            "epoch:7, mean_cost:0.596112, duration:0.160656\n",
            "epoch:7, validate_cost:0.562665, duration:0.160656\n",
            "Best Epoch:7, Train_Cost:0.596112, Valid_Cost:0.562665, Test_Cost:0.445757\n",
            "epoch:8, iteration:0/11, cost:0.517819\n",
            "epoch:8, mean_cost:0.545420, duration:0.165325\n",
            "epoch:8, validate_cost:0.564481, duration:0.165325\n",
            "Best Epoch:7, Train_Cost:0.596112, Valid_Cost:0.562665, Test_Cost:0.445757\n",
            "epoch:9, iteration:0/11, cost:0.430564\n",
            "epoch:9, mean_cost:0.554223, duration:0.167385\n",
            "epoch:9, validate_cost:0.558220, duration:0.167385\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:10, iteration:0/11, cost:0.542931\n",
            "epoch:10, mean_cost:0.536926, duration:0.154035\n",
            "epoch:10, validate_cost:0.568580, duration:0.154035\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:11, iteration:0/11, cost:0.590738\n",
            "epoch:11, mean_cost:0.531365, duration:0.169218\n",
            "epoch:11, validate_cost:0.560873, duration:0.169218\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:12, iteration:0/11, cost:0.580321\n",
            "epoch:12, mean_cost:0.524428, duration:0.164789\n",
            "epoch:12, validate_cost:0.565271, duration:0.164789\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:13, iteration:0/11, cost:0.523291\n",
            "epoch:13, mean_cost:0.538253, duration:0.158715\n",
            "epoch:13, validate_cost:0.560676, duration:0.158715\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:14, iteration:0/11, cost:0.571460\n",
            "epoch:14, mean_cost:0.520904, duration:0.158783\n",
            "epoch:14, validate_cost:0.569938, duration:0.158783\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:15, iteration:0/11, cost:0.383771\n",
            "epoch:15, mean_cost:0.527447, duration:0.160651\n",
            "epoch:15, validate_cost:0.568329, duration:0.160651\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:16, iteration:0/11, cost:0.419630\n",
            "epoch:16, mean_cost:0.522236, duration:0.162206\n",
            "epoch:16, validate_cost:0.584013, duration:0.162206\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:17, iteration:0/11, cost:0.628865\n",
            "epoch:17, mean_cost:0.527836, duration:0.160379\n",
            "epoch:17, validate_cost:0.568635, duration:0.160379\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:18, iteration:0/11, cost:0.553341\n",
            "epoch:18, mean_cost:0.498044, duration:0.154717\n",
            "epoch:18, validate_cost:0.579327, duration:0.154717\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "epoch:19, iteration:0/11, cost:0.436976\n",
            "epoch:19, mean_cost:0.504724, duration:0.163621\n",
            "epoch:19, validate_cost:0.577184, duration:0.163621\n",
            "Best Epoch:9, Train_Cost:0.554223, Valid_Cost:0.558220, Test_Cost:0.431871\n",
            "0.7156862745098039 0.8356164383561644 0.782051282051282 0.8079470198675496 0.641025641025641\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:0.799987\n",
            "epoch:0, mean_cost:0.698289, duration:0.160300\n",
            "epoch:0, validate_cost:0.590398, duration:0.160300\n",
            "Best Epoch:0, Train_Cost:0.698289, Valid_Cost:0.590398, Test_Cost:0.542757\n",
            "epoch:1, iteration:0/11, cost:0.647525\n",
            "epoch:1, mean_cost:0.663714, duration:0.156968\n",
            "epoch:1, validate_cost:0.561835, duration:0.156968\n",
            "Best Epoch:1, Train_Cost:0.663714, Valid_Cost:0.561835, Test_Cost:0.492826\n",
            "epoch:2, iteration:0/11, cost:0.692278\n",
            "epoch:2, mean_cost:0.616913, duration:0.159229\n",
            "epoch:2, validate_cost:0.561312, duration:0.159229\n",
            "Best Epoch:2, Train_Cost:0.616913, Valid_Cost:0.561312, Test_Cost:0.470967\n",
            "epoch:3, iteration:0/11, cost:0.604426\n",
            "epoch:3, mean_cost:0.593917, duration:0.165492\n",
            "epoch:3, validate_cost:0.548340, duration:0.165492\n",
            "Best Epoch:3, Train_Cost:0.593917, Valid_Cost:0.548340, Test_Cost:0.444203\n",
            "epoch:4, iteration:0/11, cost:0.565765\n",
            "epoch:4, mean_cost:0.571882, duration:0.164198\n",
            "epoch:4, validate_cost:0.544741, duration:0.164198\n",
            "Best Epoch:4, Train_Cost:0.571882, Valid_Cost:0.544741, Test_Cost:0.434459\n",
            "epoch:5, iteration:0/11, cost:0.390733\n",
            "epoch:5, mean_cost:0.577891, duration:0.154995\n",
            "epoch:5, validate_cost:0.544247, duration:0.154995\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:6, iteration:0/11, cost:0.708180\n",
            "epoch:6, mean_cost:0.569709, duration:0.161494\n",
            "epoch:6, validate_cost:0.551708, duration:0.161494\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:7, iteration:0/11, cost:0.704081\n",
            "epoch:7, mean_cost:0.563911, duration:0.162324\n",
            "epoch:7, validate_cost:0.545129, duration:0.162324\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:8, iteration:0/11, cost:0.415954\n",
            "epoch:8, mean_cost:0.563417, duration:0.161438\n",
            "epoch:8, validate_cost:0.553047, duration:0.161438\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:9, iteration:0/11, cost:0.611574\n",
            "epoch:9, mean_cost:0.551406, duration:0.163984\n",
            "epoch:9, validate_cost:0.546111, duration:0.163984\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:10, iteration:0/11, cost:0.523877\n",
            "epoch:10, mean_cost:0.540196, duration:0.167862\n",
            "epoch:10, validate_cost:0.547759, duration:0.167862\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:11, iteration:0/11, cost:0.526981\n",
            "epoch:11, mean_cost:0.543994, duration:0.166412\n",
            "epoch:11, validate_cost:0.564607, duration:0.166412\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:12, iteration:0/11, cost:0.594570\n",
            "epoch:12, mean_cost:0.559492, duration:0.161483\n",
            "epoch:12, validate_cost:0.556456, duration:0.161483\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:13, iteration:0/11, cost:0.480496\n",
            "epoch:13, mean_cost:0.528813, duration:0.157041\n",
            "epoch:13, validate_cost:0.549341, duration:0.157041\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:14, iteration:0/11, cost:0.605832\n",
            "epoch:14, mean_cost:0.531922, duration:0.162906\n",
            "epoch:14, validate_cost:0.559559, duration:0.162906\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:15, iteration:0/11, cost:0.583871\n",
            "epoch:15, mean_cost:0.510062, duration:0.175431\n",
            "epoch:15, validate_cost:0.555915, duration:0.175431\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:16, iteration:0/11, cost:0.466637\n",
            "epoch:16, mean_cost:0.519425, duration:0.162779\n",
            "epoch:16, validate_cost:0.578346, duration:0.162779\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:17, iteration:0/11, cost:0.545962\n",
            "epoch:17, mean_cost:0.513186, duration:0.155594\n",
            "epoch:17, validate_cost:0.566170, duration:0.155594\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:18, iteration:0/11, cost:0.595364\n",
            "epoch:18, mean_cost:0.496854, duration:0.157951\n",
            "epoch:18, validate_cost:0.568601, duration:0.157951\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "epoch:19, iteration:0/11, cost:0.475561\n",
            "epoch:19, mean_cost:0.503751, duration:0.163285\n",
            "epoch:19, validate_cost:0.613815, duration:0.163285\n",
            "Best Epoch:5, Train_Cost:0.577891, Valid_Cost:0.544247, Test_Cost:0.432208\n",
            "0.7745098039215687 0.7722772277227723 1.0 0.8715083798882681 0.5208333333333333\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:0.867053\n",
            "epoch:0, mean_cost:0.627880, duration:0.167028\n",
            "epoch:0, validate_cost:0.628708, duration:0.167028\n",
            "Best Epoch:0, Train_Cost:0.627880, Valid_Cost:0.628708, Test_Cost:0.560866\n",
            "epoch:1, iteration:0/11, cost:0.572804\n",
            "epoch:1, mean_cost:0.619373, duration:0.157803\n",
            "epoch:1, validate_cost:0.576056, duration:0.157803\n",
            "Best Epoch:1, Train_Cost:0.619373, Valid_Cost:0.576056, Test_Cost:0.496493\n",
            "epoch:2, iteration:0/11, cost:0.607520\n",
            "epoch:2, mean_cost:0.612046, duration:0.159842\n",
            "epoch:2, validate_cost:0.577065, duration:0.159842\n",
            "Best Epoch:1, Train_Cost:0.619373, Valid_Cost:0.576056, Test_Cost:0.496493\n",
            "epoch:3, iteration:0/11, cost:0.614937\n",
            "epoch:3, mean_cost:0.599033, duration:0.163764\n",
            "epoch:3, validate_cost:0.564782, duration:0.163764\n",
            "Best Epoch:3, Train_Cost:0.599033, Valid_Cost:0.564782, Test_Cost:0.457701\n",
            "epoch:4, iteration:0/11, cost:0.567252\n",
            "epoch:4, mean_cost:0.573733, duration:0.160983\n",
            "epoch:4, validate_cost:0.562568, duration:0.160983\n",
            "Best Epoch:4, Train_Cost:0.573733, Valid_Cost:0.562568, Test_Cost:0.451422\n",
            "epoch:5, iteration:0/11, cost:0.691912\n",
            "epoch:5, mean_cost:0.584204, duration:0.163604\n",
            "epoch:5, validate_cost:0.564456, duration:0.163604\n",
            "Best Epoch:4, Train_Cost:0.573733, Valid_Cost:0.562568, Test_Cost:0.451422\n",
            "epoch:6, iteration:0/11, cost:0.533210\n",
            "epoch:6, mean_cost:0.579029, duration:0.164035\n",
            "epoch:6, validate_cost:0.559851, duration:0.164035\n",
            "Best Epoch:6, Train_Cost:0.579029, Valid_Cost:0.559851, Test_Cost:0.447658\n",
            "epoch:7, iteration:0/11, cost:0.644402\n",
            "epoch:7, mean_cost:0.583468, duration:0.162892\n",
            "epoch:7, validate_cost:0.572686, duration:0.162892\n",
            "Best Epoch:6, Train_Cost:0.579029, Valid_Cost:0.559851, Test_Cost:0.447658\n",
            "epoch:8, iteration:0/11, cost:0.524914\n",
            "epoch:8, mean_cost:0.552476, duration:0.162711\n",
            "epoch:8, validate_cost:0.563409, duration:0.162711\n",
            "Best Epoch:6, Train_Cost:0.579029, Valid_Cost:0.559851, Test_Cost:0.447658\n",
            "epoch:9, iteration:0/11, cost:0.404682\n",
            "epoch:9, mean_cost:0.558567, duration:0.159133\n",
            "epoch:9, validate_cost:0.560374, duration:0.159133\n",
            "Best Epoch:6, Train_Cost:0.579029, Valid_Cost:0.559851, Test_Cost:0.447658\n",
            "epoch:10, iteration:0/11, cost:0.502087\n",
            "epoch:10, mean_cost:0.560938, duration:0.173344\n",
            "epoch:10, validate_cost:0.566745, duration:0.173344\n",
            "Best Epoch:6, Train_Cost:0.579029, Valid_Cost:0.559851, Test_Cost:0.447658\n",
            "epoch:11, iteration:0/11, cost:0.603282\n",
            "epoch:11, mean_cost:0.544272, duration:0.163011\n",
            "epoch:11, validate_cost:0.557724, duration:0.163011\n",
            "Best Epoch:11, Train_Cost:0.544272, Valid_Cost:0.557724, Test_Cost:0.437183\n",
            "epoch:12, iteration:0/11, cost:0.509202\n",
            "epoch:12, mean_cost:0.561611, duration:0.164320\n",
            "epoch:12, validate_cost:0.557011, duration:0.164320\n",
            "Best Epoch:12, Train_Cost:0.561611, Valid_Cost:0.557011, Test_Cost:0.434130\n",
            "epoch:13, iteration:0/11, cost:0.630510\n",
            "epoch:13, mean_cost:0.536250, duration:0.166480\n",
            "epoch:13, validate_cost:0.565910, duration:0.166480\n",
            "Best Epoch:12, Train_Cost:0.561611, Valid_Cost:0.557011, Test_Cost:0.434130\n",
            "epoch:14, iteration:0/11, cost:0.633673\n",
            "epoch:14, mean_cost:0.554409, duration:0.156956\n",
            "epoch:14, validate_cost:0.557689, duration:0.156956\n",
            "Best Epoch:12, Train_Cost:0.561611, Valid_Cost:0.557011, Test_Cost:0.434130\n",
            "epoch:15, iteration:0/11, cost:0.492677\n",
            "epoch:15, mean_cost:0.542564, duration:0.161014\n",
            "epoch:15, validate_cost:0.572551, duration:0.161014\n",
            "Best Epoch:12, Train_Cost:0.561611, Valid_Cost:0.557011, Test_Cost:0.434130\n",
            "epoch:16, iteration:0/11, cost:0.642959\n",
            "epoch:16, mean_cost:0.521051, duration:0.163725\n",
            "epoch:16, validate_cost:0.565658, duration:0.163725\n",
            "Best Epoch:12, Train_Cost:0.561611, Valid_Cost:0.557011, Test_Cost:0.434130\n",
            "epoch:17, iteration:0/11, cost:0.511226\n",
            "epoch:17, mean_cost:0.536092, duration:0.162341\n",
            "epoch:17, validate_cost:0.575330, duration:0.162341\n",
            "Best Epoch:12, Train_Cost:0.561611, Valid_Cost:0.557011, Test_Cost:0.434130\n",
            "epoch:18, iteration:0/11, cost:0.631160\n",
            "epoch:18, mean_cost:0.521816, duration:0.159278\n",
            "epoch:18, validate_cost:0.592914, duration:0.159278\n",
            "Best Epoch:12, Train_Cost:0.561611, Valid_Cost:0.557011, Test_Cost:0.434130\n",
            "epoch:19, iteration:0/11, cost:0.585893\n",
            "epoch:19, mean_cost:0.511088, duration:0.160127\n",
            "epoch:19, validate_cost:0.565150, duration:0.160127\n",
            "Best Epoch:12, Train_Cost:0.561611, Valid_Cost:0.557011, Test_Cost:0.434130\n",
            "0.7549019607843137 0.8354430379746836 0.8461538461538461 0.840764331210191 0.6522435897435898\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:0.639086\n",
            "epoch:0, mean_cost:0.616188, duration:0.172508\n",
            "epoch:0, validate_cost:0.592866, duration:0.172508\n",
            "Best Epoch:0, Train_Cost:0.616188, Valid_Cost:0.592866, Test_Cost:0.531348\n",
            "epoch:1, iteration:0/11, cost:0.626427\n",
            "epoch:1, mean_cost:0.624281, duration:0.160177\n",
            "epoch:1, validate_cost:0.564723, duration:0.160177\n",
            "Best Epoch:1, Train_Cost:0.624281, Valid_Cost:0.564723, Test_Cost:0.476745\n",
            "epoch:2, iteration:0/11, cost:0.651794\n",
            "epoch:2, mean_cost:0.601839, duration:0.169968\n",
            "epoch:2, validate_cost:0.566292, duration:0.169968\n",
            "Best Epoch:1, Train_Cost:0.624281, Valid_Cost:0.564723, Test_Cost:0.476745\n",
            "epoch:3, iteration:0/11, cost:0.460804\n",
            "epoch:3, mean_cost:0.557089, duration:0.162086\n",
            "epoch:3, validate_cost:0.556457, duration:0.162086\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:4, iteration:0/11, cost:0.660060\n",
            "epoch:4, mean_cost:0.585779, duration:0.169595\n",
            "epoch:4, validate_cost:0.558794, duration:0.169595\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:5, iteration:0/11, cost:0.574514\n",
            "epoch:5, mean_cost:0.561737, duration:0.165946\n",
            "epoch:5, validate_cost:0.558345, duration:0.165946\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:6, iteration:0/11, cost:0.600321\n",
            "epoch:6, mean_cost:0.560766, duration:0.162999\n",
            "epoch:6, validate_cost:0.558439, duration:0.162999\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:7, iteration:0/11, cost:0.457872\n",
            "epoch:7, mean_cost:0.554290, duration:0.161901\n",
            "epoch:7, validate_cost:0.563338, duration:0.161901\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:8, iteration:0/11, cost:0.379254\n",
            "epoch:8, mean_cost:0.540197, duration:0.163420\n",
            "epoch:8, validate_cost:0.560602, duration:0.163420\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:9, iteration:0/11, cost:0.606453\n",
            "epoch:9, mean_cost:0.532535, duration:0.163833\n",
            "epoch:9, validate_cost:0.570661, duration:0.163833\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:10, iteration:0/11, cost:0.494642\n",
            "epoch:10, mean_cost:0.531934, duration:0.162574\n",
            "epoch:10, validate_cost:0.564087, duration:0.162574\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:11, iteration:0/11, cost:0.554567\n",
            "epoch:11, mean_cost:0.556240, duration:0.170062\n",
            "epoch:11, validate_cost:0.562862, duration:0.170062\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:12, iteration:0/11, cost:0.554724\n",
            "epoch:12, mean_cost:0.524826, duration:0.159507\n",
            "epoch:12, validate_cost:0.590226, duration:0.159507\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:13, iteration:0/11, cost:0.551968\n",
            "epoch:13, mean_cost:0.524795, duration:0.154805\n",
            "epoch:13, validate_cost:0.564312, duration:0.154805\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:14, iteration:0/11, cost:0.553612\n",
            "epoch:14, mean_cost:0.516702, duration:0.156972\n",
            "epoch:14, validate_cost:0.585176, duration:0.156972\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:15, iteration:0/11, cost:0.282396\n",
            "epoch:15, mean_cost:0.501937, duration:0.166119\n",
            "epoch:15, validate_cost:0.568334, duration:0.166119\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:16, iteration:0/11, cost:0.394551\n",
            "epoch:16, mean_cost:0.496902, duration:0.165034\n",
            "epoch:16, validate_cost:0.574302, duration:0.165034\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:17, iteration:0/11, cost:0.381806\n",
            "epoch:17, mean_cost:0.515765, duration:0.156857\n",
            "epoch:17, validate_cost:0.580121, duration:0.156857\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:18, iteration:0/11, cost:0.513031\n",
            "epoch:18, mean_cost:0.485725, duration:0.160082\n",
            "epoch:18, validate_cost:0.590073, duration:0.160082\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "epoch:19, iteration:0/11, cost:0.446752\n",
            "epoch:19, mean_cost:0.469281, duration:0.155138\n",
            "epoch:19, validate_cost:0.640912, duration:0.155138\n",
            "Best Epoch:3, Train_Cost:0.557089, Valid_Cost:0.556457, Test_Cost:0.443920\n",
            "0.7843137254901961 0.7978723404255319 0.9615384615384616 0.8720930232558141 0.5849358974358975\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:0.782869\n",
            "epoch:0, mean_cost:0.678125, duration:0.161348\n",
            "epoch:0, validate_cost:0.608369, duration:0.161348\n",
            "Best Epoch:0, Train_Cost:0.678125, Valid_Cost:0.608369, Test_Cost:0.532510\n",
            "epoch:1, iteration:0/11, cost:0.581749\n",
            "epoch:1, mean_cost:0.633679, duration:0.165393\n",
            "epoch:1, validate_cost:0.561723, duration:0.165393\n",
            "Best Epoch:1, Train_Cost:0.633679, Valid_Cost:0.561723, Test_Cost:0.468026\n",
            "epoch:2, iteration:0/11, cost:0.672425\n",
            "epoch:2, mean_cost:0.601951, duration:0.158305\n",
            "epoch:2, validate_cost:0.566577, duration:0.158305\n",
            "Best Epoch:1, Train_Cost:0.633679, Valid_Cost:0.561723, Test_Cost:0.468026\n",
            "epoch:3, iteration:0/11, cost:0.778374\n",
            "epoch:3, mean_cost:0.608024, duration:0.162755\n",
            "epoch:3, validate_cost:0.551025, duration:0.162755\n",
            "Best Epoch:3, Train_Cost:0.608024, Valid_Cost:0.551025, Test_Cost:0.443033\n",
            "epoch:4, iteration:0/11, cost:0.618694\n",
            "epoch:4, mean_cost:0.571849, duration:0.170127\n",
            "epoch:4, validate_cost:0.559031, duration:0.170127\n",
            "Best Epoch:3, Train_Cost:0.608024, Valid_Cost:0.551025, Test_Cost:0.443033\n",
            "epoch:5, iteration:0/11, cost:0.680548\n",
            "epoch:5, mean_cost:0.569993, duration:0.160059\n",
            "epoch:5, validate_cost:0.550311, duration:0.160059\n",
            "Best Epoch:5, Train_Cost:0.569993, Valid_Cost:0.550311, Test_Cost:0.437131\n",
            "epoch:6, iteration:0/11, cost:0.432585\n",
            "epoch:6, mean_cost:0.570930, duration:0.165886\n",
            "epoch:6, validate_cost:0.549969, duration:0.165886\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:7, iteration:0/11, cost:0.505342\n",
            "epoch:7, mean_cost:0.554777, duration:0.162437\n",
            "epoch:7, validate_cost:0.553307, duration:0.162437\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:8, iteration:0/11, cost:0.473236\n",
            "epoch:8, mean_cost:0.548741, duration:0.169721\n",
            "epoch:8, validate_cost:0.555440, duration:0.169721\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:9, iteration:0/11, cost:0.640169\n",
            "epoch:9, mean_cost:0.546671, duration:0.155982\n",
            "epoch:9, validate_cost:0.556516, duration:0.155982\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:10, iteration:0/11, cost:0.435920\n",
            "epoch:10, mean_cost:0.541276, duration:0.162281\n",
            "epoch:10, validate_cost:0.555896, duration:0.162281\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:11, iteration:0/11, cost:0.422359\n",
            "epoch:11, mean_cost:0.543647, duration:0.170207\n",
            "epoch:11, validate_cost:0.556538, duration:0.170207\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:12, iteration:0/11, cost:0.347833\n",
            "epoch:12, mean_cost:0.534149, duration:0.161248\n",
            "epoch:12, validate_cost:0.558026, duration:0.161248\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:13, iteration:0/11, cost:0.656823\n",
            "epoch:13, mean_cost:0.533839, duration:0.164155\n",
            "epoch:13, validate_cost:0.566194, duration:0.164155\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:14, iteration:0/11, cost:0.564848\n",
            "epoch:14, mean_cost:0.532029, duration:0.167169\n",
            "epoch:14, validate_cost:0.562492, duration:0.167169\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:15, iteration:0/11, cost:0.593146\n",
            "epoch:15, mean_cost:0.532514, duration:0.165809\n",
            "epoch:15, validate_cost:0.583297, duration:0.165809\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:16, iteration:0/11, cost:0.504200\n",
            "epoch:16, mean_cost:0.516575, duration:0.163575\n",
            "epoch:16, validate_cost:0.571561, duration:0.163575\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:17, iteration:0/11, cost:0.351027\n",
            "epoch:17, mean_cost:0.499521, duration:0.153803\n",
            "epoch:17, validate_cost:0.563569, duration:0.153803\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:18, iteration:0/11, cost:0.516779\n",
            "epoch:18, mean_cost:0.502191, duration:0.157156\n",
            "epoch:18, validate_cost:0.582752, duration:0.157156\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "epoch:19, iteration:0/11, cost:0.500367\n",
            "epoch:19, mean_cost:0.505431, duration:0.165634\n",
            "epoch:19, validate_cost:0.572643, duration:0.165634\n",
            "Best Epoch:6, Train_Cost:0.570930, Valid_Cost:0.549969, Test_Cost:0.438239\n",
            "0.7450980392156863 0.8333333333333334 0.8333333333333334 0.8333333333333334 0.6458333333333334\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:0.753631\n",
            "epoch:0, mean_cost:0.761190, duration:0.161635\n",
            "epoch:0, validate_cost:0.637788, duration:0.161635\n",
            "Best Epoch:0, Train_Cost:0.761190, Valid_Cost:0.637788, Test_Cost:0.618982\n",
            "epoch:1, iteration:0/11, cost:0.714888\n",
            "epoch:1, mean_cost:0.639536, duration:0.165725\n",
            "epoch:1, validate_cost:0.596034, duration:0.165725\n",
            "Best Epoch:1, Train_Cost:0.639536, Valid_Cost:0.596034, Test_Cost:0.563912\n",
            "epoch:2, iteration:0/11, cost:0.618925\n",
            "epoch:2, mean_cost:0.619821, duration:0.159696\n",
            "epoch:2, validate_cost:0.584539, duration:0.159696\n",
            "Best Epoch:2, Train_Cost:0.619821, Valid_Cost:0.584539, Test_Cost:0.525751\n",
            "epoch:3, iteration:0/11, cost:0.719006\n",
            "epoch:3, mean_cost:0.604840, duration:0.163525\n",
            "epoch:3, validate_cost:0.556498, duration:0.163525\n",
            "Best Epoch:3, Train_Cost:0.604840, Valid_Cost:0.556498, Test_Cost:0.479442\n",
            "epoch:4, iteration:0/11, cost:0.535400\n",
            "epoch:4, mean_cost:0.595553, duration:0.161462\n",
            "epoch:4, validate_cost:0.549320, duration:0.161462\n",
            "Best Epoch:4, Train_Cost:0.595553, Valid_Cost:0.549320, Test_Cost:0.463385\n",
            "epoch:5, iteration:0/11, cost:0.620575\n",
            "epoch:5, mean_cost:0.599371, duration:0.155468\n",
            "epoch:5, validate_cost:0.545396, duration:0.155468\n",
            "Best Epoch:5, Train_Cost:0.599371, Valid_Cost:0.545396, Test_Cost:0.452029\n",
            "epoch:6, iteration:0/11, cost:0.474326\n",
            "epoch:6, mean_cost:0.569410, duration:0.164121\n",
            "epoch:6, validate_cost:0.541642, duration:0.164121\n",
            "Best Epoch:6, Train_Cost:0.569410, Valid_Cost:0.541642, Test_Cost:0.437800\n",
            "epoch:7, iteration:0/11, cost:0.507626\n",
            "epoch:7, mean_cost:0.574421, duration:0.161318\n",
            "epoch:7, validate_cost:0.541482, duration:0.161318\n",
            "Best Epoch:7, Train_Cost:0.574421, Valid_Cost:0.541482, Test_Cost:0.434007\n",
            "epoch:8, iteration:0/11, cost:0.415832\n",
            "epoch:8, mean_cost:0.547565, duration:0.163168\n",
            "epoch:8, validate_cost:0.542987, duration:0.163168\n",
            "Best Epoch:7, Train_Cost:0.574421, Valid_Cost:0.541482, Test_Cost:0.434007\n",
            "epoch:9, iteration:0/11, cost:0.495063\n",
            "epoch:9, mean_cost:0.555278, duration:0.157138\n",
            "epoch:9, validate_cost:0.541921, duration:0.157138\n",
            "Best Epoch:7, Train_Cost:0.574421, Valid_Cost:0.541482, Test_Cost:0.434007\n",
            "epoch:10, iteration:0/11, cost:0.522690\n",
            "epoch:10, mean_cost:0.556060, duration:0.164083\n",
            "epoch:10, validate_cost:0.542186, duration:0.164083\n",
            "Best Epoch:7, Train_Cost:0.574421, Valid_Cost:0.541482, Test_Cost:0.434007\n",
            "epoch:11, iteration:0/11, cost:0.553200\n",
            "epoch:11, mean_cost:0.546982, duration:0.162160\n",
            "epoch:11, validate_cost:0.543090, duration:0.162160\n",
            "Best Epoch:7, Train_Cost:0.574421, Valid_Cost:0.541482, Test_Cost:0.434007\n",
            "epoch:12, iteration:0/11, cost:0.412089\n",
            "epoch:12, mean_cost:0.525677, duration:0.163263\n",
            "epoch:12, validate_cost:0.540612, duration:0.163263\n",
            "Best Epoch:12, Train_Cost:0.525677, Valid_Cost:0.540612, Test_Cost:0.422913\n",
            "epoch:13, iteration:0/11, cost:0.559133\n",
            "epoch:13, mean_cost:0.523799, duration:0.165242\n",
            "epoch:13, validate_cost:0.558854, duration:0.165242\n",
            "Best Epoch:12, Train_Cost:0.525677, Valid_Cost:0.540612, Test_Cost:0.422913\n",
            "epoch:14, iteration:0/11, cost:0.411771\n",
            "epoch:14, mean_cost:0.535026, duration:0.157722\n",
            "epoch:14, validate_cost:0.541191, duration:0.157722\n",
            "Best Epoch:12, Train_Cost:0.525677, Valid_Cost:0.540612, Test_Cost:0.422913\n",
            "epoch:15, iteration:0/11, cost:0.570684\n",
            "epoch:15, mean_cost:0.543018, duration:0.170437\n",
            "epoch:15, validate_cost:0.548434, duration:0.170437\n",
            "Best Epoch:12, Train_Cost:0.525677, Valid_Cost:0.540612, Test_Cost:0.422913\n",
            "epoch:16, iteration:0/11, cost:0.651926\n",
            "epoch:16, mean_cost:0.538145, duration:0.164811\n",
            "epoch:16, validate_cost:0.541857, duration:0.164811\n",
            "Best Epoch:12, Train_Cost:0.525677, Valid_Cost:0.540612, Test_Cost:0.422913\n",
            "epoch:17, iteration:0/11, cost:0.565655\n",
            "epoch:17, mean_cost:0.518027, duration:0.167963\n",
            "epoch:17, validate_cost:0.546471, duration:0.167963\n",
            "Best Epoch:12, Train_Cost:0.525677, Valid_Cost:0.540612, Test_Cost:0.422913\n",
            "epoch:18, iteration:0/11, cost:0.485685\n",
            "epoch:18, mean_cost:0.508677, duration:0.155690\n",
            "epoch:18, validate_cost:0.546472, duration:0.155690\n",
            "Best Epoch:12, Train_Cost:0.525677, Valid_Cost:0.540612, Test_Cost:0.422913\n",
            "epoch:19, iteration:0/11, cost:0.439803\n",
            "epoch:19, mean_cost:0.503133, duration:0.164984\n",
            "epoch:19, validate_cost:0.548339, duration:0.164984\n",
            "Best Epoch:12, Train_Cost:0.525677, Valid_Cost:0.540612, Test_Cost:0.422913\n",
            "0.7450980392156863 0.8333333333333334 0.8333333333333334 0.8333333333333334 0.6458333333333334\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:0.634004\n",
            "epoch:0, mean_cost:0.626418, duration:0.167353\n",
            "epoch:0, validate_cost:0.586593, duration:0.167353\n",
            "Best Epoch:0, Train_Cost:0.626418, Valid_Cost:0.586593, Test_Cost:0.526340\n",
            "epoch:1, iteration:0/11, cost:0.564410\n",
            "epoch:1, mean_cost:0.619652, duration:0.164211\n",
            "epoch:1, validate_cost:0.571620, duration:0.164211\n",
            "Best Epoch:1, Train_Cost:0.619652, Valid_Cost:0.571620, Test_Cost:0.489295\n",
            "epoch:2, iteration:0/11, cost:0.600800\n",
            "epoch:2, mean_cost:0.586447, duration:0.158726\n",
            "epoch:2, validate_cost:0.572281, duration:0.158726\n",
            "Best Epoch:1, Train_Cost:0.619652, Valid_Cost:0.571620, Test_Cost:0.489295\n",
            "epoch:3, iteration:0/11, cost:0.709197\n",
            "epoch:3, mean_cost:0.557444, duration:0.168302\n",
            "epoch:3, validate_cost:0.566708, duration:0.168302\n",
            "Best Epoch:3, Train_Cost:0.557444, Valid_Cost:0.566708, Test_Cost:0.449097\n",
            "epoch:4, iteration:0/11, cost:0.500877\n",
            "epoch:4, mean_cost:0.561053, duration:0.166260\n",
            "epoch:4, validate_cost:0.560657, duration:0.166260\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:5, iteration:0/11, cost:0.573594\n",
            "epoch:5, mean_cost:0.586124, duration:0.162304\n",
            "epoch:5, validate_cost:0.561574, duration:0.162304\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:6, iteration:0/11, cost:0.625339\n",
            "epoch:6, mean_cost:0.571265, duration:0.160640\n",
            "epoch:6, validate_cost:0.563833, duration:0.160640\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:7, iteration:0/11, cost:0.418057\n",
            "epoch:7, mean_cost:0.541799, duration:0.169914\n",
            "epoch:7, validate_cost:0.567508, duration:0.169914\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:8, iteration:0/11, cost:0.454259\n",
            "epoch:8, mean_cost:0.541600, duration:0.162885\n",
            "epoch:8, validate_cost:0.564293, duration:0.162885\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:9, iteration:0/11, cost:0.452205\n",
            "epoch:9, mean_cost:0.540181, duration:0.160388\n",
            "epoch:9, validate_cost:0.590626, duration:0.160388\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:10, iteration:0/11, cost:0.644107\n",
            "epoch:10, mean_cost:0.533184, duration:0.160658\n",
            "epoch:10, validate_cost:0.563738, duration:0.160658\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:11, iteration:0/11, cost:0.427982\n",
            "epoch:11, mean_cost:0.541130, duration:0.166025\n",
            "epoch:11, validate_cost:0.574308, duration:0.166025\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:12, iteration:0/11, cost:0.537009\n",
            "epoch:12, mean_cost:0.532865, duration:0.168035\n",
            "epoch:12, validate_cost:0.625973, duration:0.168035\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:13, iteration:0/11, cost:0.527776\n",
            "epoch:13, mean_cost:0.535496, duration:0.156548\n",
            "epoch:13, validate_cost:0.578929, duration:0.156548\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:14, iteration:0/11, cost:0.579731\n",
            "epoch:14, mean_cost:0.522766, duration:0.165684\n",
            "epoch:14, validate_cost:0.584177, duration:0.165684\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:15, iteration:0/11, cost:0.545918\n",
            "epoch:15, mean_cost:0.498665, duration:0.164556\n",
            "epoch:15, validate_cost:0.575584, duration:0.164556\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:16, iteration:0/11, cost:0.402510\n",
            "epoch:16, mean_cost:0.495042, duration:0.161944\n",
            "epoch:16, validate_cost:0.574678, duration:0.161944\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:17, iteration:0/11, cost:0.566342\n",
            "epoch:17, mean_cost:0.494698, duration:0.161222\n",
            "epoch:17, validate_cost:0.580788, duration:0.161222\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:18, iteration:0/11, cost:0.565992\n",
            "epoch:18, mean_cost:0.476773, duration:0.157469\n",
            "epoch:18, validate_cost:0.595807, duration:0.157469\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "epoch:19, iteration:0/11, cost:0.237004\n",
            "epoch:19, mean_cost:0.459666, duration:0.163692\n",
            "epoch:19, validate_cost:0.597779, duration:0.163692\n",
            "Best Epoch:4, Train_Cost:0.561053, Valid_Cost:0.560657, Test_Cost:0.439393\n",
            "0.7450980392156863 0.8513513513513513 0.8076923076923077 0.8289473684210527 0.6746794871794872\n",
            "building the model ...\n",
            "constructing the optimizer ...\n",
            "done!\n",
            "loading data ...\n",
            "training start\n",
            "epoch:0, iteration:0/11, cost:0.648779\n",
            "epoch:0, mean_cost:0.654428, duration:0.165489\n",
            "epoch:0, validate_cost:0.591655, duration:0.165489\n",
            "Best Epoch:0, Train_Cost:0.654428, Valid_Cost:0.591655, Test_Cost:0.538041\n",
            "epoch:1, iteration:0/11, cost:0.687715\n",
            "epoch:1, mean_cost:0.612631, duration:0.162170\n",
            "epoch:1, validate_cost:0.587088, duration:0.162170\n",
            "Best Epoch:1, Train_Cost:0.612631, Valid_Cost:0.587088, Test_Cost:0.492958\n",
            "epoch:2, iteration:0/11, cost:0.803512\n",
            "epoch:2, mean_cost:0.599867, duration:0.167273\n",
            "epoch:2, validate_cost:0.561768, duration:0.167273\n",
            "Best Epoch:2, Train_Cost:0.599867, Valid_Cost:0.561768, Test_Cost:0.453915\n",
            "epoch:3, iteration:0/11, cost:0.622957\n",
            "epoch:3, mean_cost:0.594235, duration:0.166259\n",
            "epoch:3, validate_cost:0.563103, duration:0.166259\n",
            "Best Epoch:2, Train_Cost:0.599867, Valid_Cost:0.561768, Test_Cost:0.453915\n",
            "epoch:4, iteration:0/11, cost:0.555835\n",
            "epoch:4, mean_cost:0.588170, duration:0.165567\n",
            "epoch:4, validate_cost:0.561361, duration:0.165567\n",
            "Best Epoch:4, Train_Cost:0.588170, Valid_Cost:0.561361, Test_Cost:0.451397\n",
            "epoch:5, iteration:0/11, cost:0.661421\n",
            "epoch:5, mean_cost:0.584639, duration:0.159576\n",
            "epoch:5, validate_cost:0.574050, duration:0.159576\n",
            "Best Epoch:4, Train_Cost:0.588170, Valid_Cost:0.561361, Test_Cost:0.451397\n",
            "epoch:6, iteration:0/11, cost:0.535935\n",
            "epoch:6, mean_cost:0.573750, duration:0.161047\n",
            "epoch:6, validate_cost:0.567521, duration:0.161047\n",
            "Best Epoch:4, Train_Cost:0.588170, Valid_Cost:0.561361, Test_Cost:0.451397\n",
            "epoch:7, iteration:0/11, cost:0.551101\n",
            "epoch:7, mean_cost:0.560747, duration:0.169864\n",
            "epoch:7, validate_cost:0.563860, duration:0.169864\n",
            "Best Epoch:4, Train_Cost:0.588170, Valid_Cost:0.561361, Test_Cost:0.451397\n",
            "epoch:8, iteration:0/11, cost:0.500170\n",
            "epoch:8, mean_cost:0.563752, duration:0.162420\n",
            "epoch:8, validate_cost:0.561451, duration:0.162420\n",
            "Best Epoch:4, Train_Cost:0.588170, Valid_Cost:0.561361, Test_Cost:0.451397\n",
            "epoch:9, iteration:0/11, cost:0.505280\n",
            "epoch:9, mean_cost:0.553673, duration:0.161785\n",
            "epoch:9, validate_cost:0.561430, duration:0.161785\n",
            "Best Epoch:4, Train_Cost:0.588170, Valid_Cost:0.561361, Test_Cost:0.451397\n",
            "epoch:10, iteration:0/11, cost:0.462135\n",
            "epoch:10, mean_cost:0.553850, duration:0.159601\n",
            "epoch:10, validate_cost:0.563596, duration:0.159601\n",
            "Best Epoch:4, Train_Cost:0.588170, Valid_Cost:0.561361, Test_Cost:0.451397\n",
            "epoch:11, iteration:0/11, cost:0.682631\n",
            "epoch:11, mean_cost:0.560416, duration:0.167031\n",
            "epoch:11, validate_cost:0.559464, duration:0.167031\n",
            "Best Epoch:11, Train_Cost:0.560416, Valid_Cost:0.559464, Test_Cost:0.447742\n",
            "epoch:12, iteration:0/11, cost:0.501468\n",
            "epoch:12, mean_cost:0.553829, duration:0.168289\n",
            "epoch:12, validate_cost:0.560456, duration:0.168289\n",
            "Best Epoch:11, Train_Cost:0.560416, Valid_Cost:0.559464, Test_Cost:0.447742\n",
            "epoch:13, iteration:0/11, cost:0.502552\n",
            "epoch:13, mean_cost:0.541742, duration:0.156138\n",
            "epoch:13, validate_cost:0.568209, duration:0.156138\n",
            "Best Epoch:11, Train_Cost:0.560416, Valid_Cost:0.559464, Test_Cost:0.447742\n",
            "epoch:14, iteration:0/11, cost:0.593062\n",
            "epoch:14, mean_cost:0.541493, duration:0.159599\n",
            "epoch:14, validate_cost:0.561497, duration:0.159599\n",
            "Best Epoch:11, Train_Cost:0.560416, Valid_Cost:0.559464, Test_Cost:0.447742\n",
            "epoch:15, iteration:0/11, cost:0.537009\n",
            "epoch:15, mean_cost:0.542917, duration:0.179363\n",
            "epoch:15, validate_cost:0.559853, duration:0.179363\n",
            "Best Epoch:11, Train_Cost:0.560416, Valid_Cost:0.559464, Test_Cost:0.447742\n",
            "epoch:16, iteration:0/11, cost:0.635887\n",
            "epoch:16, mean_cost:0.558939, duration:0.163868\n",
            "epoch:16, validate_cost:0.572057, duration:0.163868\n",
            "Best Epoch:11, Train_Cost:0.560416, Valid_Cost:0.559464, Test_Cost:0.447742\n",
            "epoch:17, iteration:0/11, cost:0.661197\n",
            "epoch:17, mean_cost:0.545021, duration:0.163290\n",
            "epoch:17, validate_cost:0.558503, duration:0.163290\n",
            "Best Epoch:17, Train_Cost:0.545021, Valid_Cost:0.558503, Test_Cost:0.436951\n",
            "epoch:18, iteration:0/11, cost:0.469970\n",
            "epoch:18, mean_cost:0.527793, duration:0.156729\n",
            "epoch:18, validate_cost:0.565374, duration:0.156729\n",
            "Best Epoch:17, Train_Cost:0.545021, Valid_Cost:0.558503, Test_Cost:0.436951\n",
            "epoch:19, iteration:0/11, cost:0.507044\n",
            "epoch:19, mean_cost:0.514886, duration:0.160298\n",
            "epoch:19, validate_cost:0.562540, duration:0.160298\n",
            "Best Epoch:17, Train_Cost:0.545021, Valid_Cost:0.558503, Test_Cost:0.436951\n",
            "0.7549019607843137 0.8354430379746836 0.8461538461538461 0.840764331210191 0.6522435897435898\n",
            "[0.74607843 0.82416284 0.8525641  0.83596924 0.62628205]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v7rcRyfFYs6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}